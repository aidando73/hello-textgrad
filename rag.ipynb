{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install textgrad dspy faiss-cpu mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/14 21:21:15 INFO mlflow.tracking.fluent: Experiment with name 'textgrad-test' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"textgrad-test\")\n",
    "\n",
    "mlflow.litellm.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process and whether it is affected by the number of shirts.\n",
      "\n",
      "1. **Understand the Drying Process**: Drying shirts under the sun is typically a parallel process. Each shirt dries independently of the others, assuming there is enough space and sunlight for all shirts to be exposed equally.\n",
      "\n",
      "2. **Initial Information**: We know that 25 shirts take 1 hour to dry. This implies that each shirt, when exposed to the sun, takes 1 hour to dry.\n",
      "\n",
      "3. **Drying 30 Shirts**: Since drying is a parallel process and each shirt dries independently, adding more shirts does not increase the drying time for each shirt. Therefore, drying 30 shirts will also take 1 hour, provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight.\n",
      "\n",
      "4. **Conclusion**: The time it takes to dry 30 shirts is the same as the time it takes to dry 25 shirts, which is 1 hour, assuming all conditions remain constant (e.g., sunlight intensity, space for spreading the shirts).\n",
      "\n",
      "Thus, it will take 1 hour to dry 30 shirts under the sun.\n"
     ]
    }
   ],
   "source": [
    "import textgrad as tg\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the API key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "\n",
    "tg.set_backward_engine(\"gpt-4o\", override=True)\n",
    "\n",
    "# Step 1: Get an initial response from an LLM\n",
    "model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "question_string = (\"If it takes 1 hour to dry 25 shirts under the sun, \"\n",
    "                    \"how long will it take to dry 30 shirts under the sun? \"\n",
    "                    \"Reason step by step.\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.set_role_description(\"concise and accurate answer to the question\")\n",
    "\n",
    "optimizer = tg.TGD(parameters=[answer], verbose=1)\n",
    "\n",
    "evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
    "                           \"Evaluate any given answer to this question, \"\n",
    "                           \"be smart, logical, and very critical. \"\n",
    "                           \"Just provide concise feedback.\")\n",
    "\n",
    "loss_fn = tg.TextLoss(evaluation_instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------TextualGradientDescent------------------------\n",
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable(value=To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant., role=concise and accurate answer to the question, grads={Variable(value=To improve the concise and accurate answer to the question, consider the following feedback:\n",
       "\n",
       "1. **Clarify Assumptions**: While the answer correctly identifies that the drying process is parallel, it could benefit from explicitly stating the assumption that there is sufficient space and sunlight for all shirts. This would preemptively address any potential concerns about limitations in drying conditions.\n",
       "\n",
       "2. **Simplify Language**: The explanation could be made more concise by reducing redundancy. For instance, the phrase \"provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight\" could be streamlined to \"assuming equal exposure and no space limitations.\"\n",
       "\n",
       "3. **Address Potential Variables**: While the answer assumes constant conditions, it could briefly mention potential variables that might affect drying time, such as changes in weather or obstructions to sunlight, to demonstrate a comprehensive understanding of the drying process.\n",
       "\n",
       "4. **Use of Examples**: Incorporating a brief example or analogy could help illustrate the concept of parallel drying more clearly, making the explanation more relatable and easier to understand.\n",
       "\n",
       "5. **Focus on Key Points**: The answer could emphasize the key point that the drying time is independent of the number of shirts, as long as conditions remain constant, to reinforce the main conclusion.\n",
       "\n",
       "By addressing these points, the answer could become more concise, clear, and robust, thereby improving the objective function of providing a concise and accurate answer., role=feedback to concise and accurate answer to the question, grads=set())})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(answer)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(value=In Linux, the terms \"high memory\" and \"low memory\" refer to different regions of the system's physical memory, particularly in the context of 32-bit architectures. This distinction is primarily relevant for systems with large amounts of RAM.\n",
       "\n",
       "### Low Memory\n",
       "- **Definition**: Low memory is the portion of physical memory that is directly accessible by the kernel without any special handling.\n",
       "- **Address Range**: On 32-bit systems, low memory typically refers to the first 896 MB of RAM. This is because the Linux kernel reserves the upper 128 MB of the 4 GB address space for its own use, leaving 3 GB for user space and 1 GB for kernel space.\n",
       "- **Usage**: Low memory is used for kernel data structures, buffers, and other critical components that need to be accessed quickly and efficiently.\n",
       "\n",
       "### High Memory\n",
       "- **Definition**: High memory is the portion of physical memory that is not directly mapped into the kernel's address space.\n",
       "- **Address Range**: High memory starts just above the low memory limit and extends to the end of the physical RAM. On systems with more than 896 MB of RAM, the memory above this threshold is considered high memory.\n",
       "- **Usage**: High memory is used for user-space applications and data that do not need to be accessed directly by the kernel. The kernel accesses high memory through a process called \"paging,\" which involves temporarily mapping portions of high memory into the kernel's address space as needed.\n",
       "\n",
       "### Why the Distinction?\n",
       "- **32-bit Limitation**: The distinction between high and low memory is primarily due to the limitations of 32-bit addressing. A 32-bit address space can only directly address 4 GB of memory, and the kernel needs to reserve a portion of this space for its own use.\n",
       "- **Performance**: Accessing low memory is faster because it is always directly mapped into the kernel's address space. High memory requires additional overhead to map and unmap pages, which can impact performance.\n",
       "\n",
       "### 64-bit Systems\n",
       "On 64-bit systems, the distinction between high and low memory is largely irrelevant because the address space is vast enough to map all of physical memory directly. However, the terms may still be used conceptually to describe memory management strategies.\n",
       "\n",
       "In summary, the distinction between high and low memory in Linux is a result of the limitations of 32-bit architectures and the need to efficiently manage memory access for both the kernel and user-space applications., role=response from the language model, grads=set())"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_string = (\"what are high memory and low memory in linux?\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'why are my text messages coming up as maybe?',\n",
       " 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.',\n",
       " 'gold_doc_ids': [3956, 3957, 8034]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300, 500)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just importing dspy for the metric only\n",
    "from dspy.evaluate import SemanticF1\n",
    "import dspy\n",
    "from textgrad.engine import get_engine\n",
    "import litellm\n",
    "\n",
    "litellm.set_verbose=False\n",
    "\n",
    "engine = get_engine(\"experimental:gpt-4o\", cache=False)\n",
    "\n",
    "system_prompt = tg.Variable(\"You are a helpful assistant that can answer questions about the given context.\", role_description=\"system prompt for the LLM\", requires_grad=True)\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "model = tg.BlackboxLLM(engine=engine, system_prompt=system_prompt)\n",
    "\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "example = data[2]\n",
    "question = tg.Variable(example[\"question\"], role_description=\"question to the LLM\", requires_grad=False)\n",
    "# pred = model(question)\n",
    "\n",
    "# Compute the metric score for the prediction.\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "def evaluate_single(the_model, the_example):\n",
    "    the_example = dspy.Example(\n",
    "        question=the_example[\"question\"],\n",
    "        response=the_example[\"response\"]\n",
    "    )\n",
    "    pred = dspy.Prediction(\n",
    "        response=the_model(the_example[\"question\"])\n",
    "    )\n",
    "    score = metric(the_example, pred)\n",
    "    # print(\"Question:\\n\", example.question)\n",
    "    # print(\"\\n\\nGround truth:\\n\", example.response)\n",
    "    # print(\"\\n\\nPrediction:\\n\", pred.response)\n",
    "    # print(\"\\n\\nSemantic F1 score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Clear instances\n",
    "tqdm._instances.clear()\n",
    "\n",
    "# Reset monitor thread\n",
    "if hasattr(tqdm, 'monitor'):\n",
    "    tqdm.monitor.exit()\n",
    "    tqdm.monitor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# def evaluate(the_model):\n",
    "#     total_score = 0\n",
    "#     top_score = 0\n",
    "#     pbar = tqdm(devset)\n",
    "#     for example in pbar:\n",
    "#         score = evaluate_single(the_model, example)\n",
    "#         total_score += score\n",
    "#         top_score += 1\n",
    "#         pbar.set_description(f\"Evaluating (score: {total_score:.1f}/{top_score}, {total_score/max(1, top_score):.2%})\")\n",
    "#     return total_score / top_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(the_model):\n",
    "    total_score = 0\n",
    "    pbar = tqdm(total=len(devset), position=0, leave=True)\n",
    "\n",
    "    # Use ThreadPoolExecutor since the work is I/O bound (API calls)\n",
    "    with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_example = {\n",
    "            executor.submit(evaluate_single, the_model, example): example \n",
    "            for example in devset\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks as they finish\n",
    "        for future in as_completed(future_to_example):\n",
    "            score = future.result()\n",
    "            total_score += score\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Evaluating (score: {total_score:.1f}/{pbar.n}, {total_score/max(1, pbar.n):.2%})\")\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_score / len(devset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 149.8/300, 49.93%): 100%|██████████| 300/300 [03:31<00:00,  1.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4993400821965219"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=e9750851fde64b019ec10738c5b79e10&amp;experiment_id=561788570947907383&amp;trace_id=2fcb4b3e19c543eeacf75ff287cd4fbf&amp;experiment_id=561788570947907383&amp;trace_id=5f6222ae389444f3abd42d61fa988226&amp;experiment_id=561788570947907383&amp;trace_id=dd4ed9ae789b496d875f1fa7a6f54134&amp;experiment_id=561788570947907383&amp;trace_id=9af15e7656494327b930a1cd4d53cf05&amp;experiment_id=561788570947907383&amp;trace_id=8b664bc325ff463c93cab900373fde45&amp;experiment_id=561788570947907383&amp;trace_id=7f64a0b763704b00a072f0d58eb7cbe1&amp;experiment_id=561788570947907383&amp;trace_id=cea5146b198e4682b17979ba0149bca8&amp;experiment_id=561788570947907383&amp;trace_id=51dc6bdd0edf4f63ad349855e452531c&amp;experiment_id=561788570947907383&amp;trace_id=e8c179329c4e4f258a367649e4abc6b4&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=e9750851fde64b019ec10738c5b79e10), Trace(request_id=2fcb4b3e19c543eeacf75ff287cd4fbf), Trace(request_id=5f6222ae389444f3abd42d61fa988226), Trace(request_id=dd4ed9ae789b496d875f1fa7a6f54134), Trace(request_id=9af15e7656494327b930a1cd4d53cf05), Trace(request_id=8b664bc325ff463c93cab900373fde45), Trace(request_id=7f64a0b763704b00a072f0d58eb7cbe1), Trace(request_id=cea5146b198e4682b17979ba0149bca8), Trace(request_id=51dc6bdd0edf4f63ad349855e452531c), Trace(request_id=e8c179329c4e4f258a367649e4abc6b4)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a 32-byte FAISS index with 337 partitions, based on 28436 x 512-dim embeddings\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "max_characters = 6000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
    "    corpus = [json.loads(line)['text'][:max_characters] for line in f]\n",
    "\n",
    "embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Variable(value=Context:\n",
       " -As far as I remember, High Memory is used for application space and Low Memory for the kernel. Advantage is that (user-space) applications cant access kernel-space memory.\n",
       " -This is relevant to the Linux kernel; Im not sure how any Unix kernel handles this. The High Memory is the segment of memory that user-space programs can address. It cannot touch Low Memory. Low Memory is the segment of memory that the Linux kernel can address directly. If the kernel must access High Memory, it has to map it into its own address space first. There was a patch introduced recently that lets you control where the segment is. The tradeoff is that you can take addressable memory away from user space so that the kernel can have more memory that it does not have to map before using. Additional resources: http://tldp.org/HOWTO/KernelAnalysis-HOWTO-7.html http://linux-mm.org/HighMemory\n",
       " -HIGHMEM is a range of kernels memory space, but it is NOT memory you access but its a place where you put what you want to access. A typical 32bit Linux virtual memory map is like: 0x00000000-0xbfffffff: user process (3GB) 0xc0000000-0xffffffff: kernel space (1GB) (CPU-specific vector and whatsoever are ignored here). Linux splits the 1GB kernel space into 2 pieces, LOWMEM and HIGHMEM. The split varies from installation to installation. If an installation chooses, say, 512MB-512MB for LOW and HIGH mems, the 512MB LOWMEM (0xc0000000-0xdfffffff) is statically mapped at the kernel boot time; usually the first so many bytes of the physical memory is used for this so that virtual and physical addresses in this range have a constant offset of, say, 0xc0000000. On the other hand, the latter 512MB (HIGHMEM) has no static mapping (although you could leave pages semi-permanently mapped there, but you must do so explicitly in your driver code). Instead, pages are temporarily mapped and unmapped here so that virtual and physical addresses in this range have no consistent mapping. Typical uses of HIGHMEM include single-time data buffers.\n",
       " -The first reference to turn to is Linux Device Drivers (available both online and in book form), particularly chapter 15 which has a section on the topic. In an ideal world, every system component would be able to map all the memory it ever needs to access. And this is the case for processes on Linux and most operating systems: a 32-bit process can only access a little less than 2^32 bytes of virtual memory (in fact about 3GB on a typical Linux 32-bit architecture). It gets difficult for the kernel, which needs to be able to map the full memory of the process whose system call its executing, plus the whole physical memory, plus any other memory-mapped hardware device. So when a 32-bit kernel needs to map more than 4GB of memory, it must be compiled with high memory support. High memory is memory which is not permanently mapped in the kernels address space. (Low memory is the opposite: it is always mapped, so you can access it in the kernel simply by dereferencing a pointer.) When you access high memory from kernel code, you need to call kmap first, to obtain a pointer from a page data structure (struct page). Calling kmap works whether the page is in high or low memory. There is also kmap_atomic which has added constraints but is more efficient on multiprocessor machines because it uses finer-grained locking. The pointer obtained through kmap is a resource: it uses up address space. Once youve finished with it, you must call kunmap (or kunmap_atomic) to free that resource; then the pointer is no longer valid, and the contents of the page cant be accessed until you call kmap again.\n",
       " -/proc/meminfo will tell you how free works, but /proc/kcore can tell you what the kernel uses. From the same page: /proc/kcore This file represents the physical memory of the system and is stored in the ELF core file format. With this pseudo-file, and an unstripped kernel (/usr/src/linux/vmlinux) binary, GDB can be used to examine the current state of any kernel data structures. The total length of the file is the size of physical memory (RAM) plus 4KB. /proc/meminfo This file reports statistics about memory usage on the system. It is used by free(1) to report the amount of free and used memory (both physical and swap) on the system as well as the shared memory and buffers used by the kernel. Each line of the file consists of a parameter name, followed by a colon, the value of the parameter, and an option unit of measurement (e.g., kB). The list below describes the parameter names and the format specifier required to read the field value. Except as noted below, all of the fields have been present since at least Linux 2.6.0. Some fileds are displayed only if the kernel was configured with various options; those dependencies are noted in the list. MemTotal %lu Total usable RAM (i.e., physical RAM minus a few reserved bits and the kernel binary code). MemFree %lu The sum of LowFree+HighFree. Buffers %lu Relatively temporary storage for raw disk blocks that shouldnt get tremendously large (20MB or so). Cached %lu In-memory cache for files read from the disk (the page cache). Doesnt include SwapCached. SwapCached %lu Memory that once was swapped out, is swapped back in but still also is in the swap file. (If memory pressure is high, these pages dont need to be swapped out again because they are already in the swap file. This saves I/O.) Active %lu Memory that has been used more recently and usually not reclaimed unless absolutely necessary. Inactive %lu Memory which has been less recently used. It is more eligible to be reclaimed for other purposes. Active(anon) %lu (since Linux 2.6.28) [To be documented.] Inactive(anon) %lu (since Linux 2.6.28) [To be documented.] Active(file) %lu (since Linux 2.6.28) [To be documented.] Inactive(file) %lu (since Linux 2.6.28) [To be documented.] Unevictable %lu (since Linux 2.6.28) (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.) [To be documented.] Mlocked %lu (since Linux 2.6.28) (From Linux 2.6.28 to 2.6.30, CONFIG_UNEVICTABLE_LRU was required.) [To be documented.] HighTotal %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Total amount of highmem. Highmem is all memory above ~860MB of physical memory. Highmem areas are for use by user-space programs, or for the page cache. The kernel must use tricks to access this memory, making it slower to access than lowmem. HighFree %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Amount of free highmem. LowTotal %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Total amount of lowmem. Lowmem is memory which can be used for everything that highmem can be used for, but it is also available for the kernels use for its own data structures. Among many other things, it is where everything from Slab is allocated. Bad things happen when youre out of lowmem. LowFree %lu (Starting with Linux 2.6.19, CONFIG_HIGHMEM is required.) Amount of free lowmem. MmapCopy %lu (since Linux 2.6.29) (CONFIG_MMU is required.) [To be documented.] SwapTotal %lu Total amount of swap space available. SwapFree %lu Amount of swap space that is currently unused. Dirty %lu Memory which is waiting to get written back to the disk. Writeback %lu Memory which is actively being written back to the disk. AnonPages %lu (since Linux 2.6.18) Non-file backed pages mapped into user-space page tables. Mapped %lu Files which have been mmaped, such as libraries. Shmem %lu (since Linux 2.6.32) [To be documented.] Slab %lu In-kernel data structures cache. SReclaimable %lu (since Linux 2.6.19) Part of Slab, that might be reclaimed, such as caches. SUnreclaim %lu (since Linux 2.6.19) Part of Slab, that cannot be reclaimed on memory pressure. KernelStack %lu (since Linux 2.6.32) Amount of memory allocated to kernel stacks. PageTables %lu (since Linux 2.6.18) Amount of memory dedicated to the lowest level of page tables. Quicklists %lu (since Linux 2.6.27) (CONFIG_QUICKLIST is required.) [To be documented.] NFS_Unstable %lu (since Linux 2.6.18) NFS pages sent to the server, but not yet committed to stable storage. Bounce %lu (since Linux 2.6.18) Memory used for block device bounce buffers. WritebackTmp %lu (since Linux 2.6.26) Memory used by FUSE for temporary writeback buffers. CommitLimit %lu (since Linux 2.6.10) Based on the overcommit ratio (vm.overcommit_ratio), this is the total amount of memory currently available to be allocated on the system. This limit is adhered to only if strict overcommit accounting is enabled (mode 2 in /proc/sys/vm/overcommit_ratio). The CommitLimit is calculated using the following formula: CommitLimit = ([total RAM pages] - [total huge TLB pages]) * overcommit_ratio / 100 + [total swap pages] For example, on a system with 1GB of physical RAM and 7GB of swap with a overcommit_ratio of 30, this formula yields a CommitLimit of 7.3GB. For more details, see the memory overcommit documentation in the kernel source file Documentation/vm/overcommit-accounting. Committed_AS %lu The amount of memory presently allocated on the system. The committed memory is a sum of all of the memory which has been allocated by processes, even if it has not been used by them as of yet. A process which allocates 1GB of memory (using malloc(3) or similar), but touches only 300MB of that memory will show up as using only 300MB of memory even if it has the address space allocated for the entire 1GB. This 1GB is memory which has been committed to by the VM and can be used at any time by the allocating application. With strict overcommit enabled on the system (mode 2 /proc/sys/vm/overcommit_memory), allocations w\n",
       " \n",
       " \n",
       " Question:\n",
       " what are high memory and low memory in linux?, role=question to the LLM, grads=set()),\n",
       " Variable(value=You are a helpful assistant that can answer questions about the given context., role=system prompt for the LLM, grads=set())}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=618208e18c5c45e4b063a63cbf26af22&amp;experiment_id=561788570947907383&amp;trace_id=b33c2ab1908b4af384b816f8136aa543&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=618208e18c5c45e4b063a63cbf26af22), Trace(request_id=b33c2ab1908b4af384b816f8136aa543)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class RAG():\n",
    "    def __init__(self, model, search):\n",
    "        self.model = model\n",
    "        self.search = search\n",
    "\n",
    "    def __call__(self, question):\n",
    "        docs = self.search(question)\n",
    "        context = \"Context:\\n\"\n",
    "        for doc in docs.passages:\n",
    "            context += \"-\" + doc + \"\\n\"\n",
    "        question = tg.Variable(context + \"\\n\\nQuestion:\\n\" + question, role_description=\"question to the LLM\", requires_grad=False)\n",
    "        # print(question)\n",
    "        return self.model(question)\n",
    "\n",
    "rag = RAG(model, search)\n",
    "\n",
    "rag(\"what are high memory and low memory in linux?\").predecessors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 193.8/300, 64.61%): 100%|██████████| 300/300 [04:18<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.646081424389819"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=15fdb6e574b147a58a301f48c8226ad9&amp;experiment_id=561788570947907383&amp;trace_id=78b7651bdecc4a98a2a2be0f4412dd4d&amp;experiment_id=561788570947907383&amp;trace_id=3d1fc59ba9ef4df186d9631b8a70ba1f&amp;experiment_id=561788570947907383&amp;trace_id=51a10ab3f15a416babba355f396940d2&amp;experiment_id=561788570947907383&amp;trace_id=8b488d92a7174cd689098eb136efc5da&amp;experiment_id=561788570947907383&amp;trace_id=41727e679522409f95866abe66072638&amp;experiment_id=561788570947907383&amp;trace_id=8c9666de5e294aadbcaaa27d12553c6b&amp;experiment_id=561788570947907383&amp;trace_id=a34a26e79e174ec095b5a4f45e922b8d&amp;experiment_id=561788570947907383&amp;trace_id=aec18dbc090d4f199602f609d21a4ffd&amp;experiment_id=561788570947907383&amp;trace_id=81cbe6e36dff4445a68c581668e943fc&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=15fdb6e574b147a58a301f48c8226ad9), Trace(request_id=78b7651bdecc4a98a2a2be0f4412dd4d), Trace(request_id=3d1fc59ba9ef4df186d9631b8a70ba1f), Trace(request_id=51a10ab3f15a416babba355f396940d2), Trace(request_id=8b488d92a7174cd689098eb136efc5da), Trace(request_id=41727e679522409f95866abe66072638), Trace(request_id=8c9666de5e294aadbcaaa27d12553c6b), Trace(request_id=a34a26e79e174ec095b5a4f45e922b8d), Trace(request_id=aec18dbc090d4f199602f609d21a4ffd), Trace(request_id=81cbe6e36dff4445a68c581668e943fc)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgrad.tasks import load_task\n",
    "\n",
    "optimizer_prompt = \"\"\"\n",
    "You are part of an optimization system that improves text (i.e., variable). You will be asked to creatively and critically improve prompts, solutions to problems, code, or any other text-based variable. You will receive some feedback, and use the feedback to improve the variable. The feedback may be noisy, identify what is important and what is correct. Pay attention to the role description of the variable, and the context in which it is used. This is very important: You MUST give your response by sending the improved variable between {new_variable_start_tag} {{improved variable}} {new_variable_end_tag} tags. The text you send between the tags will directly replace the variable.\n",
    "\n",
    "\n",
    "### Glossary of tags that will be sent to you:\n",
    "# - <LM_SYSTEM_PROMPT>: The system prompt for the language model.\n",
    "# - <LM_INPUT>: The input to the language model.\n",
    "# - <LM_OUTPUT>: The output of the language model.\n",
    "# - <FEEDBACK>: The feedback to the variable.\n",
    "# - <CONVERSATION>: The conversation history.\n",
    "# - <FOCUS>: The focus of the optimization.\n",
    "# - <ROLE>: The role description of the variable.\n",
    "\"\"\"\n",
    "\n",
    "train_loader = tg.tasks.DataLoader(trainset, batch_size=3, shuffle=True)\n",
    "\n",
    "eval_model = get_engine(\"experimental:anthropic/claude-3-7-sonnet-latest\", cache=False)\n",
    "optimizer = tg.TextualGradientDescent(engine=eval_model, parameters=[system_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys prompt:  You are a helpful assistant that can answer questions about the given context.\n",
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0. Epoch 0: : 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  0.600\n",
      "0.167\n",
      "0.187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1. Epoch 0: : 1it [00:21, 21.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a specialized technical assistant with expertise in cybersecurity, computer systems, and user interfaces. When answering questions about the given context:\n",
      "\n",
      "1. Analyze the context thoroughly to provide accurate, relevant, and technically sound information.\n",
      "2. Offer clear, step-by-step instructions when explaining processes or technical procedures.\n",
      "3. Prioritize clarity and conciseness while ensuring your responses are comprehensive.\n",
      "4. Consider the user's technical knowledge level and provide explanations that balance technical accuracy with accessibility.\n",
      "5. When appropriate, suggest additional relevant information or best practices even if not explicitly asked.\n",
      "6. For security-related topics, emphasize practical risk assessment and provide balanced advice that considers both technical possibilities and real-world applicability.\n",
      "7. Define technical terms or concepts that may be unfamiliar to users.\n",
      "\n",
      "Your goal is to be not just helpful, but insightful, precise, and user-focused in addressing questions about the provided context.\n",
      "\n",
      "\n",
      "\n",
      "loss:  0.259\n",
      "0.245\n",
      "0.180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 2. Epoch 0: : 2it [01:57, 65.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a specialized technical assistant with expertise in cybersecurity, computer systems, user interfaces, operating systems, programming languages, and software development. When answering questions about the given context:\n",
      "\n",
      "1. Analyze the context thoroughly to identify key concepts, potential misconceptions, and underlying assumptions to provide accurate, relevant, and technically sound information.\n",
      "2. Offer clear, step-by-step instructions when explaining processes or technical procedures, using practical examples and analogies to illustrate complex concepts.\n",
      "3. Prioritize clarity and conciseness while ensuring your responses are comprehensive and appropriately detailed for the topic.\n",
      "4. Dynamically assess the user's technical knowledge level and provide explanations that balance technical accuracy with accessibility, adjusting your language based on context clues.\n",
      "5. When appropriate, suggest additional relevant information, best practices, or visual aids/external resources even if not explicitly asked.\n",
      "6. For security-related topics, emphasize practical risk assessment, highlight common security pitfalls, and provide balanced advice that considers both technical possibilities and real-world applicability.\n",
      "7. Define technical terms or concepts that may be unfamiliar to users, building definitions on existing knowledge to avoid overwhelming with jargon.\n",
      "8. When discussing historical technological debates or developments, provide context on the broader ecosystem including market forces, community adoption, and philosophical underpinnings.\n",
      "9. Encourage critical thinking by presenting multiple perspectives when applicable, especially for topics involving technological debates or design philosophies.\n",
      "10. Relate programming concepts to their practical applications and, where relevant, their security implications.\n",
      "\n",
      "Periodically check if your explanation is meeting the user's needs by offering opportunities for clarification. Your goal is to be not just helpful, but insightful, precise, and user-focused in addressing questions about the provided context.\n",
      "\n",
      "\n",
      "\n",
      "loss:  0.613\n",
      "0.271\n",
      "0.561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 0: : 3it [03:30, 78.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a specialized technical assistant with expertise in cybersecurity, computer systems, user interfaces, operating systems, programming languages, and software development. When answering questions about the given context:\n",
      "\n",
      "1. Analyze the context thoroughly to identify key concepts, potential misconceptions, and underlying assumptions, considering both historical evolution and current relevance of technologies to provide accurate, relevant, and technically sound information.\n",
      "\n",
      "2. Offer clear, step-by-step instructions when explaining processes or technical procedures, using practical examples and analogies to illustrate complex concepts that relate to everyday experiences.\n",
      "\n",
      "3. Prioritize clarity and conciseness while balancing them with the necessary level of detail, ensuring your responses are comprehensive, accessible, and appropriately tailored to the topic's complexity.\n",
      "\n",
      "4. Dynamically assess the user's technical knowledge level based on their language and questions, then adapt your explanations accordingly—using technical terminology with experts and simplified language with beginners.\n",
      "\n",
      "5. When appropriate, suggest additional relevant information, best practices, visual aids, or external resources even if not explicitly asked, and highlight economic and practical considerations that influence technology adoption.\n",
      "\n",
      "6. For security-related topics, emphasize practical risk assessment, highlight common security pitfalls, and provide balanced advice that considers both technical possibilities and real-world applicability, integrating security best practices across all technical domains.\n",
      "\n",
      "7. Define technical terms or concepts that may be unfamiliar to users, building definitions on existing knowledge to avoid overwhelming with jargon, and using user-centric language that makes technical concepts accessible.\n",
      "\n",
      "8. When discussing historical technological debates or developments, provide context on the broader ecosystem including market forces, standardization efforts, community adoption, and philosophical underpinnings.\n",
      "\n",
      "9. Encourage critical thinking by presenting multiple perspectives when applicable, including exploring alternatives to existing technologies and critically analyzing the pros and cons of current solutions.\n",
      "\n",
      "10. Relate programming concepts to their practical applications and security implications, considering how different technologies interact and depend on each other within broader technological ecosystems.\n",
      "\n",
      "Actively engage with users by asking clarifying questions and periodically checking if your explanation is meeting their needs. For urgent security issues, prioritize immediate, actionable advice. Encourage users to provide feedback and ask follow-up questions to create a collaborative problem-solving environment. Your goal is to be not just helpful, but insightful, precise, and user-focused in addressing questions about the provided context.\n",
      "\n",
      "\n",
      "\n",
      "loss:  1.000\n",
      "0.271\n",
      "0.212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 0: : 3it [05:09, 103.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a specialized technical assistant with expertise in cybersecurity, computer systems, user interfaces, operating systems, programming languages, and software development. When answering questions about the given context:\n",
      "\n",
      "1. Thoroughly analyze the context to identify the user's intent, key concepts, potential misconceptions, and underlying assumptions. Consider both the historical evolution and current relevance of technologies to provide accurate, technically sound information that directly addresses the user's specific needs.\n",
      "\n",
      "2. Offer clear, step-by-step instructions when explaining processes or technical procedures, using practical examples and analogies to illustrate complex concepts. For technical processes, anticipate common errors and include troubleshooting tips proactively.\n",
      "\n",
      "3. Prioritize clarity and conciseness while balancing them with the necessary level of detail. Ensure your responses are comprehensive, accessible, and appropriately tailored to the topic's complexity and the user's immediate needs.\n",
      "\n",
      "4. Dynamically assess the user's technical knowledge level based on their language and questions, then adapt your explanations accordingly—using technical terminology with experts and simplified explanations with beginners. Regularly check if your level of detail is appropriate by asking for feedback.\n",
      "\n",
      "5. Suggest relevant information, best practices, visual aids, or external resources that directly complement your answer. Consider the accessibility of suggested resources (especially for users with connectivity limitations) and highlight economic and practical considerations that influence technology adoption.\n",
      "\n",
      "6. For all topics, highlight security and privacy implications, emphasizing practical risk assessment and common security pitfalls. Provide balanced advice that considers both technical possibilities and real-world applicability, integrating security best practices even when not explicitly requested.\n",
      "\n",
      "7. Define technical terms or concepts that may be unfamiliar, building definitions on existing knowledge and using analogies or everyday examples to make complex concepts accessible. Break down jargon progressively to avoid overwhelming users.\n",
      "\n",
      "8. When discussing historical technological developments, provide context on the broader ecosystem including market forces, standardization efforts, community adoption, philosophical underpinnings, and ethical considerations.\n",
      "\n",
      "9. Encourage critical thinking by presenting multiple perspectives, exploring alternatives to existing technologies, and analyzing the pros and cons of current solutions. Help users make informed decisions based on their specific circumstances.\n",
      "\n",
      "10. Relate programming concepts to their practical applications and security implications, considering how different technologies interact within broader technological ecosystems. Focus on real-world impact rather than theoretical concepts alone.\n",
      "\n",
      "Actively engage with users by asking clarifying questions when their request is ambiguous or lacks detail. Verify understanding by occasionally asking users to confirm if your explanation meets their needs or if they require further clarification. For urgent security issues, prioritize immediate, actionable advice while explaining the risks involved. Create a collaborative problem-solving environment by encouraging feedback and follow-up questions. Empathize with users' challenges, particularly in stressful situations, and adjust your tone accordingly. Your goal is to be insightful, precise, and user-focused in addressing questions about the provided context while ensuring users can effectively implement your advice in their specific situation.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 0. Epoch 1: : 0it [01:02, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected dict_keys(['reasoning', 'ground_truth_key_ideas', 'system_response_key_ideas', 'discussion', 'recall', 'precision']) but got dict_keys(['reasoning', 'ground_truth_key_ideas'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/adapters/chat_adapter.py:42\u001b[0m, in \u001b[0;36mChatAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/adapters/base.py:34\u001b[0m, in \u001b[0;36mAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     output, output_logprobs \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m], output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 34\u001b[0m value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(value\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mset\u001b[39m(signature\u001b[38;5;241m.\u001b[39moutput_fields\u001b[38;5;241m.\u001b[39mkeys()):\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/utils/callback.py:266\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/adapters/chat_adapter.py:106\u001b[0m, in \u001b[0;36mChatAdapter.parse\u001b[0;34m(self, signature, completion)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m!=\u001b[39m signature\u001b[38;5;241m.\u001b[39moutput_fields\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignature\u001b[38;5;241m.\u001b[39moutput_fields\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfields\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fields\n",
      "\u001b[0;31mValueError\u001b[0m: Expected dict_keys(['reasoning', 'ground_truth_key_ideas', 'system_response_key_ideas', 'discussion', 'recall', 'precision']) but got dict_keys(['reasoning', 'ground_truth_key_ideas'])",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m batch:\n\u001b[1;32m     29\u001b[0m     response \u001b[38;5;241m=\u001b[39m rag(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 30\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m tg\u001b[38;5;241m.\u001b[39mVariable(\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m         role_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     34\u001b[0m         requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     35\u001b[0m         predecessors\u001b[38;5;241m=\u001b[39m[response]\n\u001b[1;32m     36\u001b[0m     )\n\u001b[1;32m     37\u001b[0m     losses\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "Cell \u001b[0;32mIn[138], line 13\u001b[0m, in \u001b[0;36mevaluate_response\u001b[0;34m(response, the_example)\u001b[0m\n\u001b[1;32m      6\u001b[0m the_example \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mExample(\n\u001b[1;32m      7\u001b[0m     question\u001b[38;5;241m=\u001b[39mthe_example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      8\u001b[0m     response\u001b[38;5;241m=\u001b[39mthe_example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m pred \u001b[38;5;241m=\u001b[39m dspy\u001b[38;5;241m.\u001b[39mPrediction(\n\u001b[1;32m     11\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthe_example\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# print(\"Question:\\n\", example.question)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# print(\"\\n\\nGround truth:\\n\", example.response)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# print(\"\\n\\nPrediction:\\n\", pred.response)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# print(\"\\n\\nSemantic F1 score:\", score)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/utils/callback.py:266\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/primitives/program.py:22\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/evaluate/auto_evaluation.py:48\u001b[0m, in \u001b[0;36mSemanticF1.forward\u001b[0;34m(self, example, pred, trace)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, example, pred, trace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mground_truth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexample\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     score \u001b[38;5;241m=\u001b[39m f1_score(scores\u001b[38;5;241m.\u001b[39mprecision, scores\u001b[38;5;241m.\u001b[39mrecall)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score \u001b[38;5;28;01mif\u001b[39;00m trace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/utils/callback.py:266\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/primitives/program.py:22\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/predict/chain_of_thought.py:20\u001b[0m, in \u001b[0;36mChainOfThought.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/utils/callback.py:266\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/predict/predict.py:73\u001b[0m, in \u001b[0;36mPredict.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;129m@with_callbacks\u001b[39m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/predict/predict.py:100\u001b[0m, in \u001b[0;36mPredict.forward\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARNING: Not all input fields were provided to module. Present: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpresent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Missing: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     99\u001b[0m adapter \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39madapter \u001b[38;5;129;01mor\u001b[39;00m ChatAdapter()\n\u001b[0;32m--> 100\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdemos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m pred \u001b[38;5;241m=\u001b[39m Prediction\u001b[38;5;241m.\u001b[39mfrom_completions(completions, signature\u001b[38;5;241m=\u001b[39msignature)\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m settings\u001b[38;5;241m.\u001b[39mtrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/adapters/chat_adapter.py:48\u001b[0m, in \u001b[0;36mChatAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# fallback to JSONAdapter\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mJSONAdapter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlm_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/adapters/json_adapter.py:61\u001b[0m, in \u001b[0;36mJSONAdapter.__call__\u001b[0;34m(self, lm, lm_kwargs, signature, demos, inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m values \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs:\n\u001b[0;32m---> 61\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(value\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(\n\u001b[1;32m     63\u001b[0m         signature\u001b[38;5;241m.\u001b[39moutput_fields\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     64\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignature\u001b[38;5;241m.\u001b[39moutput_fields\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m     values\u001b[38;5;241m.\u001b[39mappend(value)\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/utils/callback.py:266\u001b[0m, in \u001b[0;36mwith_callbacks.<locals>.wrapper\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# If no callbacks are provided, just call the function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callbacks:\n\u001b[0;32m--> 266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;66;03m# Generate call ID as the unique identifier for the call, this is useful for instrumentation.\u001b[39;00m\n\u001b[1;32m    269\u001b[0m call_id \u001b[38;5;241m=\u001b[39m uuid\u001b[38;5;241m.\u001b[39muuid4()\u001b[38;5;241m.\u001b[39mhex\n",
      "File \u001b[0;32m~/dev/hello-textgrad/env/lib/python3.10/site-packages/dspy/adapters/json_adapter.py:107\u001b[0m, in \u001b[0;36mJSONAdapter.parse\u001b[0;34m(self, signature, completion)\u001b[0m\n\u001b[1;32m    104\u001b[0m         fields[k] \u001b[38;5;241m=\u001b[39m parse_value(v, signature\u001b[38;5;241m.\u001b[39moutput_fields[k]\u001b[38;5;241m.\u001b[39mannotation)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fields\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m!=\u001b[39m signature\u001b[38;5;241m.\u001b[39moutput_fields\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignature\u001b[38;5;241m.\u001b[39moutput_fields\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfields\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fields\n",
      "\u001b[0;31mValueError\u001b[0m: Expected dict_keys(['reasoning', 'ground_truth_key_ideas', 'system_response_key_ideas', 'discussion', 'recall', 'precision']) but got dict_keys(['reasoning', 'ground_truth_key_ideas'])"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=ec6c37a65c9e432c99f70dbee8b74b78&amp;experiment_id=561788570947907383&amp;trace_id=5945e7bb3a0b4cb792170747a38a403f&amp;experiment_id=561788570947907383&amp;trace_id=a19fa33c871e4eca8735509040282817&amp;experiment_id=561788570947907383&amp;trace_id=5cdc48e6b8074f649f75ccb1f466918b&amp;experiment_id=561788570947907383&amp;trace_id=896217178ff94e01acff6ae0001647ce&amp;experiment_id=561788570947907383&amp;trace_id=bc448cb291fb4fef9906b40509ce0d6e&amp;experiment_id=561788570947907383&amp;trace_id=172e02d32abd489da7ab599782904413&amp;experiment_id=561788570947907383&amp;trace_id=54947a842ff449fc98cfb8a9f536d3d4&amp;experiment_id=561788570947907383&amp;trace_id=13e2dfb25b9b4d828180b1d617015757&amp;experiment_id=561788570947907383&amp;trace_id=bb5273376d894deea21d4ae90b76b2d1&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=ec6c37a65c9e432c99f70dbee8b74b78), Trace(request_id=5945e7bb3a0b4cb792170747a38a403f), Trace(request_id=a19fa33c871e4eca8735509040282817), Trace(request_id=5cdc48e6b8074f649f75ccb1f466918b), Trace(request_id=896217178ff94e01acff6ae0001647ce), Trace(request_id=bc448cb291fb4fef9906b40509ce0d6e), Trace(request_id=172e02d32abd489da7ab599782904413), Trace(request_id=54947a842ff449fc98cfb8a9f536d3d4), Trace(request_id=13e2dfb25b9b4d828180b1d617015757), Trace(request_id=bb5273376d894deea21d4ae90b76b2d1)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 3\n",
    "\n",
    "print(\"sys prompt: \", system_prompt)\n",
    "\n",
    "def evaluate_response(response, the_example):\n",
    "    the_example = dspy.Example(\n",
    "        question=the_example[\"question\"],\n",
    "        response=the_example[\"response\"]\n",
    "    )\n",
    "    pred = dspy.Prediction(\n",
    "        response=response.value\n",
    "    )\n",
    "    score = metric(the_example, pred)\n",
    "    # print(\"Question:\\n\", example.question)\n",
    "    # print(\"\\n\\nGround truth:\\n\", example.response)\n",
    "    # print(\"\\n\\nPrediction:\\n\", pred.response)\n",
    "    # print(\"\\n\\nSemantic F1 score:\", score)\n",
    "    return score\n",
    "\n",
    "\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS}\")\n",
    "    pbar = tqdm(train_loader, position=0)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        pbar.set_description(f\"Training step {step}. Epoch {epoch}\")\n",
    "        optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for example in batch:\n",
    "            response = rag(example[\"question\"])\n",
    "            score = evaluate_response(response, example)\n",
    "            loss = tg.Variable(\n",
    "                f\"{1 - score:.3f}\",\n",
    "                role_description=\"loss\",\n",
    "                requires_grad=True,\n",
    "                predecessors=[response]\n",
    "            )\n",
    "            losses.append(loss)\n",
    "        loss = tg.sum(losses)\n",
    "        # print(\"loss: \", loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(\"\\n\\n\")\n",
    "        print(\"sys prompt: \", system_prompt)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        if step == 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 170.6/289, 59.02%):  96%|█████████▋| 289/300 [04:37<00:07,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "# print(system_prompt)\n",
    "\n",
    "engine = get_engine(\"experimental:gpt-4o\", cache=False)\n",
    "model = tg.BlackboxLLM(engine, system_prompt=system_prompt)\n",
    "rag = RAG(model, search)\n",
    "\n",
    "evaluate(rag)\n",
    "# rag(\"what are high memory and low memory in linux?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
