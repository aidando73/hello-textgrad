{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install textgrad dspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process and whether it is affected by the number of shirts.\n",
      "\n",
      "1. **Understand the Drying Process**: Drying shirts under the sun is typically a parallel process. Each shirt dries independently of the others, assuming there is enough space and sunlight for all shirts to be exposed equally.\n",
      "\n",
      "2. **Initial Information**: We know that 25 shirts take 1 hour to dry. This implies that each shirt, when exposed to the sun, takes 1 hour to dry.\n",
      "\n",
      "3. **Drying 30 Shirts**: Since drying is a parallel process and each shirt dries independently, adding more shirts does not increase the drying time for each shirt. Therefore, drying 30 shirts will also take 1 hour, provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight.\n",
      "\n",
      "4. **Conclusion**: The time it takes to dry 30 shirts is the same as the time it takes to dry 25 shirts, which is 1 hour, assuming all conditions remain constant (e.g., sunlight intensity, space for spreading the shirts).\n",
      "\n",
      "Thus, it will take 1 hour to dry 30 shirts under the sun.\n"
     ]
    }
   ],
   "source": [
    "import textgrad as tg\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the API key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "\n",
    "tg.set_backward_engine(\"gpt-4o\", override=True)\n",
    "\n",
    "# Step 1: Get an initial response from an LLM\n",
    "model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "question_string = (\"If it takes 1 hour to dry 25 shirts under the sun, \"\n",
    "                    \"how long will it take to dry 30 shirts under the sun? \"\n",
    "                    \"Reason step by step.\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.set_role_description(\"concise and accurate answer to the question\")\n",
    "\n",
    "optimizer = tg.TGD(parameters=[answer], verbose=1)\n",
    "\n",
    "evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
    "                           \"Evaluate any given answer to this question, \"\n",
    "                           \"be smart, logical, and very critical. \"\n",
    "                           \"Just provide concise feedback.\")\n",
    "\n",
    "loss_fn = tg.TextLoss(evaluation_instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------TextualGradientDescent------------------------\n",
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable(value=To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant., role=concise and accurate answer to the question, grads={Variable(value=To improve the concise and accurate answer to the question, consider the following feedback:\n",
       "\n",
       "1. **Clarify Assumptions**: While the answer correctly identifies that the drying process is parallel, it could benefit from explicitly stating the assumption that there is sufficient space and sunlight for all shirts. This would preemptively address any potential concerns about limitations in drying conditions.\n",
       "\n",
       "2. **Simplify Language**: The explanation could be made more concise by reducing redundancy. For instance, the phrase \"provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight\" could be streamlined to \"assuming equal exposure and no space limitations.\"\n",
       "\n",
       "3. **Address Potential Variables**: While the answer assumes constant conditions, it could briefly mention potential variables that might affect drying time, such as changes in weather or obstructions to sunlight, to demonstrate a comprehensive understanding of the drying process.\n",
       "\n",
       "4. **Use of Examples**: Incorporating a brief example or analogy could help illustrate the concept of parallel drying more clearly, making the explanation more relatable and easier to understand.\n",
       "\n",
       "5. **Focus on Key Points**: The answer could emphasize the key point that the drying time is independent of the number of shirts, as long as conditions remain constant, to reinforce the main conclusion.\n",
       "\n",
       "By addressing these points, the answer could become more concise, clear, and robust, thereby improving the objective function of providing a concise and accurate answer., role=feedback to concise and accurate answer to the question, grads=set())})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(answer)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(value=In the context of Linux, \"high memory\" and \"low memory\" refer to different regions of the system's physical memory, particularly on 32-bit systems. This distinction arises from the way memory is managed and accessed by the operating system and hardware.\n",
       "\n",
       "### Low Memory\n",
       "\n",
       "- **Definition**: Low memory is the portion of physical memory that is directly accessible by the kernel without any special handling. On 32-bit systems, this typically includes the first 896 MB of RAM.\n",
       "- **Address Space**: It is mapped directly into the kernel's address space, allowing the kernel to access it easily and efficiently.\n",
       "- **Usage**: Low memory is used for kernel data structures, buffers, and other critical components that require fast and direct access by the kernel.\n",
       "- **Limitations**: The size of low memory is limited by the architecture and the kernel's address space layout, which can be a constraint on systems with large amounts of RAM.\n",
       "\n",
       "### High Memory\n",
       "\n",
       "- **Definition**: High memory refers to the portion of physical memory that is not directly mapped into the kernel's address space. On 32-bit systems, this is typically any memory above the 896 MB threshold.\n",
       "- **Address Space**: High memory requires special handling to be accessed by the kernel. It is not directly accessible and must be mapped into the kernel's address space temporarily when needed.\n",
       "- **Usage**: High memory is used for user-space applications and data that do not require constant access by the kernel. It is managed using techniques like paging to make it accessible when necessary.\n",
       "- **Handling**: The kernel uses mechanisms like page tables to map high memory into its address space as needed, which can introduce some overhead compared to low memory access.\n",
       "\n",
       "### 64-bit Systems\n",
       "\n",
       "On 64-bit systems, the distinction between high and low memory is less relevant because the address space is large enough to map all of physical memory directly into the kernel's address space. This eliminates the need for special handling of high memory, simplifying memory management.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "The distinction between high and low memory is primarily a concern for 32-bit systems with large amounts of RAM. It reflects the limitations of the 32-bit address space and the need for efficient memory management techniques to handle more memory than can be directly addressed. On modern 64-bit systems, these concerns are largely mitigated by the expanded address space., role=response from the language model, grads=set())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_string = (\"what are high memory and low memory oi linux?\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'why are my text messages coming up as maybe?',\n",
       " 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.',\n",
       " 'gold_doc_ids': [3956, 3957, 8034]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just importing dspy for the metric only\n",
    "from dspy.evaluate import SemanticF1\n",
    "import dspy\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "example = data[2]\n",
    "question = tg.Variable(example[\"question\"], role_description=\"question to the LLM\", requires_grad=False)\n",
    "pred = model(question)\n",
    "\n",
    "# Compute the metric score for the prediction.\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "def evaluate_single(the_model, the_example):\n",
    "    the_example = dspy.Example(\n",
    "        question=the_example[\"question\"],\n",
    "        response=the_example[\"response\"]\n",
    "    )\n",
    "    question = tg.Variable(the_example[\"question\"], role_description=\"question to the LLM\", requires_grad=False)\n",
    "    pred = dspy.Prediction(\n",
    "        response=the_model(question)\n",
    "    )\n",
    "    score = metric(the_example, pred)\n",
    "    # print(\"Question:\\n\", example.question)\n",
    "    # print(\"\\n\\nGround truth:\\n\", example.response)\n",
    "    # print(\"\\n\\nPrediction:\\n\", pred.response)\n",
    "    # print(\"\\n\\nSemantic F1 score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 0/0, 0.00%):   0%|          | 0/300 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(the_model):\n",
    "    total_score = 0\n",
    "    top_score = 0\n",
    "    for example in tqdm(devset, desc=f\"Evaluating (score: {total_score}/{top_score}, {total_score/max(1, top_score):.2%})\"):\n",
    "        score = evaluate_single(the_model, example)\n",
    "        total_score += score\n",
    "        top_score += 1\n",
    "    return total_score / top_score\n",
    "\n",
    "evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
