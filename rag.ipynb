{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install textgrad dspy faiss-cpu mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.litellm.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process and whether it is affected by the number of shirts.\n",
      "\n",
      "1. **Understand the Drying Process**: Drying shirts under the sun is typically a parallel process. Each shirt dries independently of the others, assuming there is enough space and sunlight for all shirts to be exposed equally.\n",
      "\n",
      "2. **Initial Information**: We know that 25 shirts take 1 hour to dry. This implies that each shirt, when exposed to the sun, takes 1 hour to dry.\n",
      "\n",
      "3. **Drying 30 Shirts**: Since drying is a parallel process and each shirt dries independently, adding more shirts does not increase the drying time for each shirt. Therefore, drying 30 shirts will also take 1 hour, provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight.\n",
      "\n",
      "4. **Conclusion**: The time it takes to dry 30 shirts is the same as the time it takes to dry 25 shirts, which is 1 hour, assuming all conditions remain constant (e.g., sunlight intensity, space for spreading the shirts).\n",
      "\n",
      "Thus, it will take 1 hour to dry 30 shirts under the sun.\n"
     ]
    }
   ],
   "source": [
    "import textgrad as tg\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the API key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "\n",
    "tg.set_backward_engine(\"gpt-4o\", override=True)\n",
    "\n",
    "# Step 1: Get an initial response from an LLM\n",
    "model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "question_string = (\"If it takes 1 hour to dry 25 shirts under the sun, \"\n",
    "                    \"how long will it take to dry 30 shirts under the sun? \"\n",
    "                    \"Reason step by step.\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.set_role_description(\"concise and accurate answer to the question\")\n",
    "\n",
    "optimizer = tg.TGD(parameters=[answer], verbose=1)\n",
    "\n",
    "evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
    "                           \"Evaluate any given answer to this question, \"\n",
    "                           \"be smart, logical, and very critical. \"\n",
    "                           \"Just provide concise feedback.\")\n",
    "\n",
    "loss_fn = tg.TextLoss(evaluation_instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------TextualGradientDescent------------------------\n",
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable(value=To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant., role=concise and accurate answer to the question, grads={Variable(value=To improve the concise and accurate answer to the question, consider the following feedback:\n",
       "\n",
       "1. **Clarify Assumptions**: While the answer correctly identifies that the drying process is parallel, it could benefit from explicitly stating the assumption that there is sufficient space and sunlight for all shirts. This would preemptively address any potential concerns about limitations in drying conditions.\n",
       "\n",
       "2. **Simplify Language**: The explanation could be made more concise by reducing redundancy. For instance, the phrase \"provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight\" could be streamlined to \"assuming equal exposure and no space limitations.\"\n",
       "\n",
       "3. **Address Potential Variables**: While the answer assumes constant conditions, it could briefly mention potential variables that might affect drying time, such as changes in weather or obstructions to sunlight, to demonstrate a comprehensive understanding of the drying process.\n",
       "\n",
       "4. **Use of Examples**: Incorporating a brief example or analogy could help illustrate the concept of parallel drying more clearly, making the explanation more relatable and easier to understand.\n",
       "\n",
       "5. **Focus on Key Points**: The answer could emphasize the key point that the drying time is independent of the number of shirts, as long as conditions remain constant, to reinforce the main conclusion.\n",
       "\n",
       "By addressing these points, the answer could become more concise, clear, and robust, thereby improving the objective function of providing a concise and accurate answer., role=feedback to concise and accurate answer to the question, grads=set())})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(answer)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_string = (\"what are high memory and low memory in linux?\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'why are my text messages coming up as maybe?',\n",
       " 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.',\n",
       " 'gold_doc_ids': [3956, 3957, 8034]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300, 500)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just importing dspy for the metric only\n",
    "from dspy.evaluate import SemanticF1\n",
    "import dspy\n",
    "from textgrad.engine import get_engine\n",
    "import litellm\n",
    "\n",
    "litellm.set_verbose=False\n",
    "\n",
    "engine = get_engine(\"experimental:gpt-4o\", cache=False)\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "model = tg.BlackboxLLM(engine=engine)\n",
    "\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "example = data[2]\n",
    "question = tg.Variable(example[\"question\"], role_description=\"question to the LLM\", requires_grad=False)\n",
    "pred = model(question)\n",
    "\n",
    "# Compute the metric score for the prediction.\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "def evaluate_single(the_model, the_example):\n",
    "    the_example = dspy.Example(\n",
    "        question=the_example[\"question\"],\n",
    "        response=the_example[\"response\"]\n",
    "    )\n",
    "    question = tg.Variable(the_example[\"question\"], role_description=\"question to the LLM\", requires_grad=False)\n",
    "    pred = dspy.Prediction(\n",
    "        response=the_model(question)\n",
    "    )\n",
    "    score = metric(the_example, pred)\n",
    "    # print(\"Question:\\n\", example.question)\n",
    "    # print(\"\\n\\nGround truth:\\n\", example.response)\n",
    "    # print(\"\\n\\nPrediction:\\n\", pred.response)\n",
    "    # print(\"\\n\\nSemantic F1 score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Clear instances\n",
    "tqdm._instances.clear()\n",
    "\n",
    "# Reset monitor thread\n",
    "if hasattr(tqdm, 'monitor'):\n",
    "    tqdm.monitor.exit()\n",
    "    tqdm.monitor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# def evaluate(the_model):\n",
    "#     total_score = 0\n",
    "#     top_score = 0\n",
    "#     pbar = tqdm(devset)\n",
    "#     for example in pbar:\n",
    "#         score = evaluate_single(the_model, example)\n",
    "#         total_score += score\n",
    "#         top_score += 1\n",
    "#         pbar.set_description(f\"Evaluating (score: {total_score:.1f}/{top_score}, {total_score/max(1, top_score):.2%})\")\n",
    "#     return total_score / top_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(the_model):\n",
    "    total_score = 0\n",
    "    pbar = tqdm(total=len(devset), position=0, leave=True)\n",
    "\n",
    "    # Use ThreadPoolExecutor since the work is I/O bound (API calls)\n",
    "    with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_example = {\n",
    "            executor.submit(evaluate_single, the_model, example): example \n",
    "            for example in devset\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks as they finish\n",
    "        for future in as_completed(future_to_example):\n",
    "            score = future.result()\n",
    "            total_score += score\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Evaluating (score: {total_score:.1f}/{pbar.n}, {total_score/max(1, pbar.n):.2%})\")\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_score / len(devset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 39.6/82, 48.25%):  27%|██▋       | 82/301 [01:50<04:54,  1.35s/it] \n",
      "Evaluating (score: 134.0/300, 44.66%): 100%|██████████| 300/300 [02:03<00:00,  2.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.446588789809674"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a 32-byte FAISS index with 337 partitions, based on 28436 x 512-dim embeddings\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "max_characters = 6000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
    "    corpus = [json.loads(line)['text'][:max_characters] for line in f]\n",
    "\n",
    "embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(value=In Linux, the concepts of \"high memory\" and \"low memory\" relate specifically to how a 32-bit Linux kernel manages memory. This is particularly relevant when the physical memory installed in a system exceeds the addressable range of a 32-bit address space.\n",
       "\n",
       "### Low Memory\n",
       "- **Definition**: Low memory refers to the portion of memory that the kernel can directly address without any special mapping.\n",
       "- **Limits**: In a 32-bit architecture, low memory roughly covers up to 896 MB of the physical memory space (due to address space required for other kernel functionalities), although the exact cutoff can vary based on kernel configuration.\n",
       "- **Access**: Kernel operations on low memory are straightforward because this memory is permanently mapped into the kernel's address space.\n",
       "- **Use**: Low memory is used for kernel operations and data structures because it doesn't require additional handling for access, making operations more efficient.\n",
       "\n",
       "### High Memory\n",
       "- **Definition**: High memory refers to the portion of memory that is above the low memory threshold and not directly accessible by the kernel without additional mapping.\n",
       "- **Limits**: High memory is everything above the direct low memory range and up to the limits of the installed RAM.\n",
       "- **Access**: Accessing high memory requires the kernel to map it into its address space using mechanisms like `kmap()` or `kmap_atomic()`. These functions map high memory pages into the kernel's address space temporarily.\n",
       "- **Use**: Primarily used for user-space programs and data, or for disk caches, since the kernel can’t directly access it without using mapping functions.\n",
       "\n",
       "### Trade-offs and Configurations\n",
       "- **Memory Mapping**: Using `kmap()` introduces overhead, so frequent access to high memory can slow down kernel operations.\n",
       "- **Kernel Configuration**: You can configure the division of memory between high and low (like shifting how much is low vs. high memory) depending on specific workload requirements, although doing this requires recompiling the kernel or using specific patches.\n",
       "\n",
       "### Contextual Importance\n",
       "- **32-bit Limitation**: This discussion is specific to 32-bit systems. Modern 64-bit systems don't distinguish between high and low memory in the same way because they have a much larger addressable space.\n",
       "- **Performance**: The performance impact of using high memory over low memory can be significant depending on the workload and how frequently high memory needs to be accessed.\n",
       "\n",
       "In summary, the distinction between high and low memory in Linux is mainly about optimizing access on systems where the physical memory exceeds what a 32-bit address space can handle without special management., role=response from the language model, grads=set())"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RAG():\n",
    "    def __init__(self, model, search):\n",
    "        self.model = model\n",
    "        self.search = search\n",
    "\n",
    "    def __call__(self, question):\n",
    "        docs = self.search(question)\n",
    "        context = \"\\n\".join(docs.passages)\n",
    "        question = tg.Variable(context + \"\\n\" + question, role_description=\"question to the LLM\", requires_grad=False)\n",
    "        # print(question)\n",
    "        return self.model(question)\n",
    "\n",
    "\n",
    "rag = RAG(model, search)\n",
    "\n",
    "rag(\"what are high memory and low memory in linux?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
