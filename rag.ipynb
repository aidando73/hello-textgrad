{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install textgrad dspy faiss-cpu mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/14 21:21:15 INFO mlflow.tracking.fluent: Experiment with name 'textgrad-test' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"textgrad-test\")\n",
    "\n",
    "mlflow.litellm.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process and whether it is affected by the number of shirts.\n",
      "\n",
      "1. **Understand the Drying Process**: Drying shirts under the sun is typically a parallel process. Each shirt dries independently of the others, assuming there is enough space and sunlight for all shirts to be exposed equally.\n",
      "\n",
      "2. **Initial Information**: We know that 25 shirts take 1 hour to dry. This implies that each shirt, when exposed to the sun, takes 1 hour to dry.\n",
      "\n",
      "3. **Drying 30 Shirts**: Since drying is a parallel process and each shirt dries independently, adding more shirts does not increase the drying time for each shirt. Therefore, drying 30 shirts will also take 1 hour, provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight.\n",
      "\n",
      "4. **Conclusion**: The time it takes to dry 30 shirts is the same as the time it takes to dry 25 shirts, which is 1 hour, assuming all conditions remain constant (e.g., sunlight intensity, space for spreading the shirts).\n",
      "\n",
      "Thus, it will take 1 hour to dry 30 shirts under the sun.\n"
     ]
    }
   ],
   "source": [
    "import textgrad as tg\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify the API key is loaded\n",
    "if os.getenv(\"OPENAI_API_KEY\") is None:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found in environment variables\")\n",
    "\n",
    "\n",
    "tg.set_backward_engine(\"gpt-4o\", override=True)\n",
    "\n",
    "# Step 1: Get an initial response from an LLM\n",
    "model = tg.BlackboxLLM(\"gpt-4o\")\n",
    "question_string = (\"If it takes 1 hour to dry 25 shirts under the sun, \"\n",
    "                    \"how long will it take to dry 30 shirts under the sun? \"\n",
    "                    \"Reason step by step.\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer.set_role_description(\"concise and accurate answer to the question\")\n",
    "\n",
    "optimizer = tg.TGD(parameters=[answer], verbose=1)\n",
    "\n",
    "evaluation_instruction = (f\"Here's a question: {question_string}. \"\n",
    "                           \"Evaluate any given answer to this question, \"\n",
    "                           \"be smart, logical, and very critical. \"\n",
    "                           \"Just provide concise feedback.\")\n",
    "\n",
    "loss_fn = tg.TextLoss(evaluation_instruction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------TextualGradientDescent------------------------\n",
      "To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable(value=To determine how long it will take to dry 30 shirts under the sun, we need to consider the drying process. Drying shirts under the sun is a parallel process, meaning each shirt dries independently, assuming there is sufficient space and sunlight for all shirts. Given that 25 shirts take 1 hour to dry, each shirt takes 1 hour to dry. Therefore, drying 30 shirts will also take 1 hour, assuming equal exposure and no space limitations. The drying time is independent of the number of shirts as long as conditions remain constant., role=concise and accurate answer to the question, grads={Variable(value=To improve the concise and accurate answer to the question, consider the following feedback:\n",
       "\n",
       "1. **Clarify Assumptions**: While the answer correctly identifies that the drying process is parallel, it could benefit from explicitly stating the assumption that there is sufficient space and sunlight for all shirts. This would preemptively address any potential concerns about limitations in drying conditions.\n",
       "\n",
       "2. **Simplify Language**: The explanation could be made more concise by reducing redundancy. For instance, the phrase \"provided that all shirts have equal exposure to sunlight and there is no limitation in space or sunlight\" could be streamlined to \"assuming equal exposure and no space limitations.\"\n",
       "\n",
       "3. **Address Potential Variables**: While the answer assumes constant conditions, it could briefly mention potential variables that might affect drying time, such as changes in weather or obstructions to sunlight, to demonstrate a comprehensive understanding of the drying process.\n",
       "\n",
       "4. **Use of Examples**: Incorporating a brief example or analogy could help illustrate the concept of parallel drying more clearly, making the explanation more relatable and easier to understand.\n",
       "\n",
       "5. **Focus on Key Points**: The answer could emphasize the key point that the drying time is independent of the number of shirts, as long as conditions remain constant, to reinforce the main conclusion.\n",
       "\n",
       "By addressing these points, the answer could become more concise, clear, and robust, thereby improving the objective function of providing a concise and accurate answer., role=feedback to concise and accurate answer to the question, grads=set())})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = loss_fn(answer)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(value=In Linux, the terms \"high memory\" and \"low memory\" refer to different regions of the system's physical memory, particularly in the context of 32-bit architectures. This distinction is primarily relevant for systems with large amounts of RAM.\n",
       "\n",
       "### Low Memory\n",
       "- **Definition**: Low memory is the portion of physical memory that is directly accessible by the kernel without any special handling.\n",
       "- **Address Range**: On 32-bit systems, low memory typically refers to the first 896 MB of RAM. This is because the Linux kernel reserves the upper 128 MB of the 4 GB address space for its own use, leaving 3 GB for user space and 1 GB for kernel space.\n",
       "- **Usage**: Low memory is used for kernel data structures, buffers, and other critical components that need to be accessed quickly and efficiently.\n",
       "\n",
       "### High Memory\n",
       "- **Definition**: High memory is the portion of physical memory that is not directly mapped into the kernel's address space.\n",
       "- **Address Range**: High memory starts just above the low memory limit and extends to the end of the physical RAM. On systems with more than 896 MB of RAM, the memory above this threshold is considered high memory.\n",
       "- **Usage**: High memory is used for user-space applications and data that do not need to be accessed directly by the kernel. The kernel accesses high memory through a process called \"paging,\" which involves temporarily mapping portions of high memory into the kernel's address space as needed.\n",
       "\n",
       "### Why the Distinction?\n",
       "- **32-bit Limitation**: The distinction between high and low memory is primarily due to the limitations of 32-bit addressing. A 32-bit address space can only directly address 4 GB of memory, and the kernel needs to reserve a portion of this space for its own use.\n",
       "- **Performance**: Accessing low memory is faster because it is always directly mapped into the kernel's address space. High memory requires additional overhead to map and unmap pages, which can impact performance.\n",
       "\n",
       "### 64-bit Systems\n",
       "On 64-bit systems, the distinction between high and low memory is largely irrelevant because the address space is vast enough to map all of physical memory directly. However, the terms may still be used conceptually to describe memory management strategies.\n",
       "\n",
       "In summary, the distinction between high and low memory in Linux is a result of the limitations of 32-bit architectures and the need to efficiently manage memory access for both the kernel and user-space applications., role=response from the language model, grads=set())"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_string = (\"what are high memory and low memory in linux?\")\n",
    "\n",
    "question = tg.Variable(question_string, role_description=\"question to the LLM\", requires_grad=False)\n",
    "\n",
    "# Step 2: Get the LLM's response\n",
    "answer = model(question)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'why are my text messages coming up as maybe?',\n",
       " 'response': 'This is part of the Proactivity features new with iOS 9: It looks at info in emails to see if anyone with this number sent you an email and if it finds the phone number associated with a contact from your email, it will show you \"Maybe\". \\n\\nHowever, it has been suggested there is a bug in iOS 11.2 that can result in \"Maybe\" being displayed even when \"Find Contacts in Other Apps\" is disabled.',\n",
       " 'gold_doc_ids': [3956, 3957, 8034]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"ragqa_arena_tech_examples.jsonl\") as f:\n",
    "    data = [json.loads(line) for line in f]\n",
    "\n",
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 300, 500)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.Random(0).shuffle(data)\n",
    "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
    "\n",
    "len(trainset), len(devset), len(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just importing dspy for the metric only\n",
    "from dspy.evaluate import SemanticF1\n",
    "import dspy\n",
    "from textgrad.engine import get_engine\n",
    "import litellm\n",
    "\n",
    "litellm.set_verbose=False\n",
    "\n",
    "engine = get_engine(\"experimental:gpt-4o\", cache=False)\n",
    "\n",
    "system_prompt = tg.Variable(\"You are a helpful assistant that can answer questions about the given context.\", role_description=\"system prompt for the LLM\", requires_grad=True)\n",
    "\n",
    "# Instantiate the metric.\n",
    "metric = SemanticF1(decompositional=True)\n",
    "model = tg.BlackboxLLM(engine=engine, system_prompt=system_prompt)\n",
    "\n",
    "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
    "example = data[2]\n",
    "question = tg.Variable(example[\"question\"], role_description=\"question to the LLM\", requires_grad=False)\n",
    "# pred = model(question)\n",
    "\n",
    "# Compute the metric score for the prediction.\n",
    "lm = dspy.LM('openai/gpt-4o-mini')\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "def evaluate_single(the_model, the_example):\n",
    "    the_example = dspy.Example(\n",
    "        question=the_example[\"question\"],\n",
    "        response=the_example[\"response\"]\n",
    "    )\n",
    "    pred = dspy.Prediction(\n",
    "        response=the_model(the_example[\"question\"])\n",
    "    )\n",
    "    score = metric(the_example, pred)\n",
    "    # print(\"Question:\\n\", example.question)\n",
    "    # print(\"\\n\\nGround truth:\\n\", example.response)\n",
    "    # print(\"\\n\\nPrediction:\\n\", pred.response)\n",
    "    # print(\"\\n\\nSemantic F1 score:\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Clear instances\n",
    "tqdm._instances.clear()\n",
    "\n",
    "# Reset monitor thread\n",
    "if hasattr(tqdm, 'monitor'):\n",
    "    tqdm.monitor.exit()\n",
    "    tqdm.monitor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# def evaluate(the_model):\n",
    "#     total_score = 0\n",
    "#     top_score = 0\n",
    "#     pbar = tqdm(devset)\n",
    "#     for example in pbar:\n",
    "#         score = evaluate_single(the_model, example)\n",
    "#         total_score += score\n",
    "#         top_score += 1\n",
    "#         pbar.set_description(f\"Evaluating (score: {total_score:.1f}/{top_score}, {total_score/max(1, top_score):.2%})\")\n",
    "#     return total_score / top_score\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate(the_model):\n",
    "    total_score = 0\n",
    "    pbar = tqdm(total=len(devset), position=0, leave=True)\n",
    "\n",
    "    # Use ThreadPoolExecutor since the work is I/O bound (API calls)\n",
    "    with ThreadPoolExecutor(max_workers=24) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_example = {\n",
    "            executor.submit(evaluate_single, the_model, example): example \n",
    "            for example in devset\n",
    "        }\n",
    "        \n",
    "        # Process completed tasks as they finish\n",
    "        for future in as_completed(future_to_example):\n",
    "            score = future.result()\n",
    "            total_score += score\n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Evaluating (score: {total_score:.1f}/{pbar.n}, {total_score/max(1, pbar.n):.2%})\")\n",
    "    \n",
    "    pbar.close()\n",
    "    return total_score / len(devset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 149.8/300, 49.93%): 100%|██████████| 300/300 [03:31<00:00,  1.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4993400821965219"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=e9750851fde64b019ec10738c5b79e10&amp;experiment_id=561788570947907383&amp;trace_id=2fcb4b3e19c543eeacf75ff287cd4fbf&amp;experiment_id=561788570947907383&amp;trace_id=5f6222ae389444f3abd42d61fa988226&amp;experiment_id=561788570947907383&amp;trace_id=dd4ed9ae789b496d875f1fa7a6f54134&amp;experiment_id=561788570947907383&amp;trace_id=9af15e7656494327b930a1cd4d53cf05&amp;experiment_id=561788570947907383&amp;trace_id=8b664bc325ff463c93cab900373fde45&amp;experiment_id=561788570947907383&amp;trace_id=7f64a0b763704b00a072f0d58eb7cbe1&amp;experiment_id=561788570947907383&amp;trace_id=cea5146b198e4682b17979ba0149bca8&amp;experiment_id=561788570947907383&amp;trace_id=51dc6bdd0edf4f63ad349855e452531c&amp;experiment_id=561788570947907383&amp;trace_id=e8c179329c4e4f258a367649e4abc6b4&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=e9750851fde64b019ec10738c5b79e10), Trace(request_id=2fcb4b3e19c543eeacf75ff287cd4fbf), Trace(request_id=5f6222ae389444f3abd42d61fa988226), Trace(request_id=dd4ed9ae789b496d875f1fa7a6f54134), Trace(request_id=9af15e7656494327b930a1cd4d53cf05), Trace(request_id=8b664bc325ff463c93cab900373fde45), Trace(request_id=7f64a0b763704b00a072f0d58eb7cbe1), Trace(request_id=cea5146b198e4682b17979ba0149bca8), Trace(request_id=51dc6bdd0edf4f63ad349855e452531c), Trace(request_id=e8c179329c4e4f258a367649e4abc6b4)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a 32-byte FAISS index with 337 partitions, based on 28436 x 512-dim embeddings\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "max_characters = 6000  # for truncating >99th percentile of documents\n",
    "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
    "\n",
    "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
    "    corpus = [json.loads(line)['text'][:max_characters] for line in f]\n",
    "\n",
    "embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\n",
    "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable(value=In Linux, high memory and low memory refer to different segments of the system's memory space, specifically in the context of how a 32-bit Linux kernel manages its physical and virtual address space.\n",
       "\n",
       "1. **Low Memory:**\n",
       "   - Low memory is the portion of the system's memory that is directly accessible by the Linux kernel without any additional mapping. \n",
       "   - It is always mapped into the kernel's address space, allowing the kernel to access it easily by simply using a pointer.\n",
       "   - This space is crucial for the kernel's operations as it uses low memory for kernel data structures and operations that need quick access.\n",
       "   - In a typical 32-bit system with a 4GB addressable memory space, about 1GB is reserved for the kernel, split into low and high memory.\n",
       "   - The low memory area can vary in size depending on the configuration (e.g., 512MB for low memory).\n",
       "\n",
       "2. **High Memory:**\n",
       "   - High memory refers to the portion of memory that is not directly mapped into the kernel's address space and is primarily used for user-space applications and data.\n",
       "   - When the kernel needs to access high memory, it must first map it into its address space using functions like `kmap`.\n",
       "   - This need to map high memory makes accessing it slower compared to low memory.\n",
       "   - High memory is used for purposes such as data buffers and file caches, which do not need the same fast access as kernel-critical data.\n",
       "\n",
       "The distinction between high and low memory becomes significant in systems where the physical memory exceeds the limitations of addressable space in the kernel, especially in 32-bit systems. High memory allows these systems to utilize more memory than what is directly accessible without mapping efforts. This method also ensures that user-space applications cannot directly access kernel memory, providing a layer of security and stability., role=response from the language model, grads=set())"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=6de715d40f454be486d4c32b9679f82a&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "Trace(request_id=6de715d40f454be486d4c32b9679f82a)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class RAG():\n",
    "    def __init__(self, model, search):\n",
    "        self.model = model\n",
    "        self.search = search\n",
    "\n",
    "    def __call__(self, question):\n",
    "        docs = self.search(question)\n",
    "        context = \"Context:\\n\"\n",
    "        for doc in docs.passages:\n",
    "            context += \"-\" + doc + \"\\n\"\n",
    "        question = tg.Variable(context + \"\\n\\nQuestion:\\n\" + question, role_description=\"question to the LLM\", requires_grad=False)\n",
    "        # print(question)\n",
    "        return self.model(question)\n",
    "\n",
    "rag = RAG(model, search)\n",
    "\n",
    "rag(\"what are high memory and low memory in linux?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 193.8/300, 64.61%): 100%|██████████| 300/300 [04:18<00:00,  1.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.646081424389819"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=15fdb6e574b147a58a301f48c8226ad9&amp;experiment_id=561788570947907383&amp;trace_id=78b7651bdecc4a98a2a2be0f4412dd4d&amp;experiment_id=561788570947907383&amp;trace_id=3d1fc59ba9ef4df186d9631b8a70ba1f&amp;experiment_id=561788570947907383&amp;trace_id=51a10ab3f15a416babba355f396940d2&amp;experiment_id=561788570947907383&amp;trace_id=8b488d92a7174cd689098eb136efc5da&amp;experiment_id=561788570947907383&amp;trace_id=41727e679522409f95866abe66072638&amp;experiment_id=561788570947907383&amp;trace_id=8c9666de5e294aadbcaaa27d12553c6b&amp;experiment_id=561788570947907383&amp;trace_id=a34a26e79e174ec095b5a4f45e922b8d&amp;experiment_id=561788570947907383&amp;trace_id=aec18dbc090d4f199602f609d21a4ffd&amp;experiment_id=561788570947907383&amp;trace_id=81cbe6e36dff4445a68c581668e943fc&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=15fdb6e574b147a58a301f48c8226ad9), Trace(request_id=78b7651bdecc4a98a2a2be0f4412dd4d), Trace(request_id=3d1fc59ba9ef4df186d9631b8a70ba1f), Trace(request_id=51a10ab3f15a416babba355f396940d2), Trace(request_id=8b488d92a7174cd689098eb136efc5da), Trace(request_id=41727e679522409f95866abe66072638), Trace(request_id=8c9666de5e294aadbcaaa27d12553c6b), Trace(request_id=a34a26e79e174ec095b5a4f45e922b8d), Trace(request_id=aec18dbc090d4f199602f609d21a4ffd), Trace(request_id=81cbe6e36dff4445a68c581668e943fc)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textgrad.tasks import load_task\n",
    "\n",
    "optimizer_prompt = \"\"\"\n",
    "You are part of an optimization system that improves text (i.e., variable). You will be asked to creatively and critically improve prompts, solutions to problems, code, or any other text-based variable. You will receive some feedback, and use the feedback to improve the variable. The feedback may be noisy, identify what is important and what is correct. Pay attention to the role description of the variable, and the context in which it is used. This is very important: You MUST give your response by sending the improved variable between {new_variable_start_tag} {{improved variable}} {new_variable_end_tag} tags. The text you send between the tags will directly replace the variable.\n",
    "\n",
    "\n",
    "### Glossary of tags that will be sent to you:\n",
    "# - <LM_SYSTEM_PROMPT>: The system prompt for the language model.\n",
    "# - <LM_INPUT>: The input to the language model.\n",
    "# - <LM_OUTPUT>: The output of the language model.\n",
    "# - <FEEDBACK>: The feedback to the variable.\n",
    "# - <CONVERSATION>: The conversation history.\n",
    "# - <FOCUS>: The focus of the optimization.\n",
    "# - <ROLE>: The role description of the variable.\n",
    "\"\"\"\n",
    "\n",
    "train_loader = tg.tasks.DataLoader(trainset, batch_size=3, shuffle=True)\n",
    "\n",
    "eval_model = get_engine(\"experimental:anthropic/claude-3-7-sonnet-latest\", cache=False)\n",
    "optimizer = tg.TextualGradientDescent(engine=eval_model, parameters=[system_prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys prompt:  You are a helpful assistant that can answer questions about the given context.\n",
      "Epoch 0/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1. Epoch 0: : 1it [00:34, 34.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant that can answer questions about the given context. Your responses should be accurate, informative, and directly based on the information provided. If the answer is not contained in the context, acknowledge this limitation clearly rather than making up information. Provide concise yet thorough answers, highlighting key information and maintaining a helpful tone. When appropriate, structure your responses for clarity using bullet points or paragraphs.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 2. Epoch 0: : 2it [01:23, 43.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative responses. When answering questions, aim to be thorough yet concise. Structure your responses for optimal readability using bullet points for lists, paragraphs for explanations, and headers when organizing complex information. Provide examples where appropriate, and if you're uncertain about something, acknowledge your limitations rather than providing potentially incorrect information. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 0: : 3it [02:19, 49.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and trustworthy responses. Communicate clearly and concisely while being friendly and engaging. When you don't know something, acknowledge your limitations rather than making up information. Avoid providing harmful, illegal, unethical, or deceptive information. Consider the context and nuance of each query, and provide balanced perspectives on complex topics. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 0: : 3it [03:03, 61.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Your goal is to be helpful, harmless, and honest. Provide detailed explanations when appropriate, but be concise when brevity is needed. Avoid making up information or presenting speculation as fact. When uncertain, acknowledge the limits of your knowledge. Break down complex topics into understandable components, using analogies and examples where helpful. Maintain a respectful, friendly tone and adapt your language to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1. Epoch 1: : 1it [00:45, 45.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 2. Epoch 1: : 2it [01:02, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 1: : 3it [01:39, 32.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 1: : 3it [01:55, 38.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 1. Epoch 2: : 1it [00:25, 25.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 2. Epoch 2: : 2it [01:07, 35.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 2: : 3it [01:38, 33.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training step 3. Epoch 2: : 3it [02:10, 43.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "sys prompt:  You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div>\n",
       "  <style scoped>\n",
       "  button {\n",
       "    border: none;\n",
       "    border-radius: 4px;\n",
       "    background-color: rgb(34, 114, 180);\n",
       "    font-family: -apple-system, \"system-ui\", \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial;\n",
       "    font-size: 13px;\n",
       "    color: white;\n",
       "    margin-top: 8px;\n",
       "    margin-bottom: 8px;\n",
       "    padding: 8px 16px;\n",
       "    cursor: pointer;\n",
       "  }\n",
       "  button:hover {\n",
       "    background-color: rgb(66, 153, 224);\n",
       "  }\n",
       "  </style>\n",
       "  <button\n",
       "    onclick=\"\n",
       "        const display = this.nextElementSibling.style.display;\n",
       "        const isCollapsed = display === 'none';\n",
       "        this.nextElementSibling.style.display = isCollapsed ? null : 'none';\n",
       "\n",
       "        const verb = isCollapsed ? 'Collapse' : 'Expand';\n",
       "        this.innerText = `${verb} MLflow Trace`;\n",
       "    \"\n",
       "  >Collapse MLflow Trace</button>\n",
       "  <iframe\n",
       "    id=\"trace-renderer\"\n",
       "    style=\"width: 100%; height: 500px; border: none; resize: vertical;\"\n",
       "    src=\"http://localhost:5000/static-files/lib/notebook-trace-renderer/index.html?trace_id=22fa237cbd844c1884a7b3e2d945ab56&amp;experiment_id=561788570947907383&amp;trace_id=ed2a4a0862e84942bae252448d36b91e&amp;experiment_id=561788570947907383&amp;trace_id=259c94ba210c40c99a448f0a7c2af52a&amp;experiment_id=561788570947907383&amp;trace_id=c33e16c49956497e8a7232b8743366f9&amp;experiment_id=561788570947907383&amp;trace_id=817507d6c75e4b4482a402f07f33c6c9&amp;experiment_id=561788570947907383&amp;trace_id=b46401a7ebd843c5b52de686cea79213&amp;experiment_id=561788570947907383&amp;trace_id=2c255269e9b04b26aee869cfd76c4d10&amp;experiment_id=561788570947907383&amp;trace_id=99fbe3c83505421f9471b9bcaa05499a&amp;experiment_id=561788570947907383&amp;trace_id=69ebef28503a43f4a2c12aaa2df7361f&amp;experiment_id=561788570947907383&amp;trace_id=b6e8fb6cef8844e5ba261a974e0b35ef&amp;experiment_id=561788570947907383&amp;version=2.20.4\"\n",
       "  />\n",
       "</div>\n"
      ],
      "text/plain": [
       "[Trace(request_id=22fa237cbd844c1884a7b3e2d945ab56), Trace(request_id=ed2a4a0862e84942bae252448d36b91e), Trace(request_id=259c94ba210c40c99a448f0a7c2af52a), Trace(request_id=c33e16c49956497e8a7232b8743366f9), Trace(request_id=817507d6c75e4b4482a402f07f33c6c9), Trace(request_id=b46401a7ebd843c5b52de686cea79213), Trace(request_id=2c255269e9b04b26aee869cfd76c4d10), Trace(request_id=99fbe3c83505421f9471b9bcaa05499a), Trace(request_id=69ebef28503a43f4a2c12aaa2df7361f), Trace(request_id=b6e8fb6cef8844e5ba261a974e0b35ef)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOTAL_EPOCHS = 3\n",
    "\n",
    "print(\"sys prompt: \", system_prompt)\n",
    "\n",
    "results = []\n",
    "\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    print(f\"Epoch {epoch}/{TOTAL_EPOCHS}\")\n",
    "    pbar = tqdm(train_loader, position=0)\n",
    "    for step, batch in enumerate(pbar):\n",
    "        pbar.set_description(f\"Training step {step}. Epoch {epoch}\")\n",
    "        optimizer.zero_grad()\n",
    "        losses = []\n",
    "        for example in batch:\n",
    "            score = evaluate_single(rag, example)\n",
    "            loss = tg.Variable(f\"{1 - score:.3f}\", role_description=\"loss\", requires_grad=True)\n",
    "            losses.append(loss)\n",
    "        loss = tg.sum(losses)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        print(\"\\n\\n\")\n",
    "        print(\"sys prompt: \", system_prompt)\n",
    "        print(\"\\n\\n\")\n",
    "        results.append(system_prompt)\n",
    "\n",
    "        if step == 3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant designed to provide accurate, informative, and thoughtful responses to user queries. Strive to be clear, concise, and comprehensive in your explanations. When appropriate, provide examples to illustrate concepts. If a question is ambiguous, ask for clarification rather than making assumptions. If you're unsure about something, acknowledge the limitations of your knowledge rather than providing potentially incorrect information. Maintain a respectful and supportive tone throughout all interactions. Tailor your responses to the user's level of understanding. Prioritize clarity and helpfulness in all your interactions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating (score: 95.6/150, 63.75%):  50%|█████     | 150/300 [01:52<01:26,  1.73it/s]"
     ]
    }
   ],
   "source": [
    "print(system_prompt)\n",
    "\n",
    "model = tg.BlackboxLLM(\"gpt-4o\", system_prompt=system_prompt)\n",
    "rag = RAG(model, search)\n",
    "\n",
    "evaluate(rag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
